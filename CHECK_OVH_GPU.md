# VÃ©rification GPU OVH pour LLaVA 13B

**Date**: 2025-01-23  
**Question**: Le modÃ¨le LLaVA 13B peut-il Ãªtre installÃ© sur le serveur OVH ?

---

## ðŸ” **VÃ©rifications NÃ©cessaires**

### **1. VÃ©rifier la VRAM disponible**
```bash
# Sur le serveur OVH
nvidia-smi
```

**Output attendu** :
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.10    Driver Version: 535.86.10    CUDA Version: 12.2   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   45C    P8    10W /  70W |   8192MiB / 15360MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

### **2. InterprÃ©tation des RÃ©sultats**

| GPU Type | VRAM Totale | LLaVA 7B | LLaVA 13B | LLaVA 34B |
|----------|-------------|----------|-----------|----------|
| **Tesla T4** | 15 GB | âœ… OK (8 GB) | âœ… OK (16 GB) | âŒ Non (32 GB) |
| **Tesla V100** | 32 GB | âœ… OK (8 GB) | âœ… OK (16 GB) | âœ… OK (32 GB) |
| **RTX 4090** | 24 GB | âœ… OK (8 GB) | âœ… OK (16 GB) | âŒ Non (32 GB) |
| **RTX 3080** | 10 GB | âœ… OK (8 GB) | âŒ Non (16 GB) | âŒ Non (32 GB) |

---

## ðŸ“Š **Exigences LLaVA 13B**

### **VRAM Requise**
- **Minimum** : 16 GB VRAM
- **RecommandÃ©** : 20 GB VRAM (marge de sÃ©curitÃ©)
- **Taille modÃ¨le** : ~8 GB (tÃ©lÃ©chargement)

### **RAM SystÃ¨me**
- **Minimum** : 32 GB RAM
- **RecommandÃ©** : 64 GB RAM

### **CPU**
- **Minimum** : 8 cÅ“urs
- **RecommandÃ©** : 16+ cÅ“urs

---

## ðŸ§ª **Test d'Installation**

### **Ã‰tape 1 : VÃ©rifier Ollama**
```bash
# Sur le serveur OVH
ollama list
```

### **Ã‰tape 2 : Tester le tÃ©lÃ©chargement**
```bash
# Commencer le tÃ©lÃ©chargement (peut prendre 10-15 minutes)
ollama pull llava:13b
```

### **Ã‰tape 3 : VÃ©rifier l'installation**
```bash
# Lister les modÃ¨les installÃ©s
ollama list

# Tester le modÃ¨le
ollama run llava:13b "Hello, can you see this text?"
```

---

## âš ï¸ **Risques Potentiels**

### **1. VRAM Insuffisante**
- **SymptÃ´me** : `CUDA out of memory` lors du chargement
- **Solution** : Revenir Ã  LLaVA 7B ou utiliser un GPU plus puissant

### **2. RAM SystÃ¨me Insuffisante**
- **SymptÃ´me** : Processus tuÃ© par le systÃ¨me (OOM Killer)
- **Solution** : Augmenter la RAM ou utiliser un modÃ¨le plus petit

### **3. Performance DÃ©gradÃ©e**
- **SymptÃ´me** : RÃ©ponses trÃ¨s lentes (>5 minutes)
- **Solution** : Optimiser les paramÃ¨tres ou revenir Ã  7B

---

## ðŸ“‹ **Checklist de VÃ©rification**

### **Avant Installation**
- [ ] `nvidia-smi` â†’ VRAM â‰¥ 16 GB
- [ ] `free -h` â†’ RAM â‰¥ 32 GB
- [ ] `nproc` â†’ CPU â‰¥ 8 cÅ“urs
- [ ] `df -h` â†’ Espace disque â‰¥ 20 GB

### **Pendant Installation**
- [ ] `ollama pull llava:13b` â†’ Pas d'erreur
- [ ] TÃ©lÃ©chargement complet (~8 GB)
- [ ] Pas de message "out of memory"

### **AprÃ¨s Installation**
- [ ] `ollama list` â†’ `llava:13b` prÃ©sent
- [ ] `ollama run llava:13b` â†’ RÃ©ponse rapide (<30s)
- [ ] Test avec image â†’ GÃ©nÃ©ration correcte

---

## ðŸŽ¯ **Recommandation**

### **Si VRAM â‰¥ 16 GB** â†’ **Installer LLaVA 13B** âœ…
- Meilleure qualitÃ©
- Moins d'erreurs de coordonnÃ©es
- Vitesse acceptable (200-300s)

### **Si VRAM < 16 GB** â†’ **Garder LLaVA 7B** âš ï¸
- QualitÃ© moyenne mais fonctionnelle
- Rapide (100-150s)
- Stable

---

## ðŸ“ž **Actions ImmÃ©diates**

1. **Connectez-vous au serveur OVH**
2. **ExÃ©cutez** : `nvidia-smi`
3. **Partagez le rÃ©sultat** pour vÃ©rification
4. **Si OK** â†’ `ollama pull llava:13b`
5. **Si erreur** â†’ Garder LLaVA 7B

---

**Pouvez-vous vÃ©rifier la VRAM sur votre serveur OVH ?** ðŸ”
